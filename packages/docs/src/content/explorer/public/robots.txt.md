Le fichier `robots.txt` contient les règles d’exclusion des robots qui indexent les sites internet.

<doc-alert type="warning">
Il n’est pas recommandé d’éditer ce fichier.
</doc-alert>

## Configuration par défaut

Par défaut, le fichier contient la configuration suivante :

```bash
User-agent: *
Disallow:
```

Cette configuration autorise les robots à indexer toutes les pages du site.

## Plus d’informations

Vous pouvez retrouver plus d’informations sur la documentation de [Google](https://developers.google.com/search/docs/advanced/robots/create-robots-txt?hl=fr).
